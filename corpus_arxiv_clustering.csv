Titre,Auteur,Date,URL/ID,Texte,Type
Cluster algebras generated by projective cluster variables,"Karin Baur, Alireza Nasr-Isfahani",2020/11/07,http://arxiv.org/abs/2011.03720v2,"We introduce the notion of a lower bound cluster algebra generated byprojective cluster variables as a polynomial ring over the initial clustervariables and the so-called projective cluster variables. We show that under anacyclicity assumption, the cluster algebra and the lower bound cluster algebragenerated by projective cluster variables coincide. In this case we use ourresults to construct a basis for the cluster algebra. We also show that anycoefficient-free cluster algebra of types $A_n$ or $\widetilde{A}_n$ is equalto the corresponding lower bound cluster algebra generated by projectivecluster variables.",ArXiv
On Convex Clustering Solutions,"Canh Hao Nguyen, Hiroshi Mamitsuka",2021/05/18,http://arxiv.org/abs/2105.08348v1,"Convex clustering is an attractive clustering algorithm with favorableproperties such as efficiency and optimality owing to its convex formulation.It is thought to generalize both k-means clustering and agglomerativeclustering. However, it is not known whether convex clustering preservesdesirable properties of these algorithms. A common expectation is that convexclustering may learn difficult cluster types such as non-convex ones. Currentunderstanding of convex clustering is limited to only consistency results onwell-separated clusters. We show new understanding of its solutions. We provethat convex clustering can only learn convex clusters. We then show that theclusters have disjoint bounding balls with significant gaps. We furthercharacterize the solutions, regularization hyperparameters, inclusterable casesand consistency.",ArXiv
Towards combinatorial clustering: preliminary research survey,Mark Sh. Levin,2015/05/28,http://arxiv.org/abs/1505.07872v1,"The paper describes clustering problems from the combinatorial viewpoint. Abrief systemic survey is presented including the following: (i) basicclustering problems (e.g., classification, clustering, sorting, clustering withan order over cluster), (ii) basic approaches to assessment of objects andobject proximities (i.e., scales, comparison, aggregation issues), (iii) basicapproaches to evaluation of local quality characteristics for clusters andtotal quality characteristics for clustering solutions, (iv) clustering asmulticriteria optimization problem, (v) generalized modular clusteringframework, (vi) basic clustering models/methods (e.g., hierarchical clustering,k-means clustering, minimum spanning tree based clustering, clustering asassignment, detection of clisue/quasi-clique based clustering, correlationclustering, network communities based clustering), Special attention istargeted to formulation of clustering as multicriteria optimization models.Combinatorial optimization models are used as auxiliary problems (e.g.,assignment, partitioning, knapsack problem, multiple choice problem,morphological clique problem, searching for consensus/median for structures).Numerical examples illustrate problem formulations, solving methods, andapplications. The material can be used as follows: (a) a research survey, (b) afundamental for designing the structure/architecture of composite modularclustering software, (c) a bibliography reference collection, and (d) atutorial.",ArXiv
Cluster automorphism groups of cluster algebras with coefficients,"Wen Chang, Bin Zhu",2015/06/05,http://arxiv.org/abs/1506.01942v1,"We study the cluster automorphism group of a skew-symmetric cluster algebrawith geometric coefficients. For this, we introduce the notion of gluing freecluster algebra, and show that under a weak condition the cluster automorphismgroup of a gluing free cluster algebra is a subgroup of the clusterautomorphism group of its principal part cluster algebra (i.e. thecorresponding cluster algebra without coefficients). We show that severalclasses of cluster algebras with coefficients are gluing free, for example,cluster algebras with principal coefficients, cluster algebras with universalgeometric coefficients, and cluster algebras from surfaces (except a 4-gon)with coefficients from boundaries. Moreover, except four kinds of surfaces, thecluster automorphism group of a cluster algebra from a surface withcoefficients from boundaries is isomorphic to the cluster automorphism group ofits principal part cluster algebra; for a cluster algebra with principalcoefficients, its cluster automorphism group is isomorphic to the automorphismgroup of its initial quiver.",ArXiv
K-expectiles clustering,"Bingling Wang, Yinxing Li, Wolfgang Karl Härdle",2021/03/16,http://arxiv.org/abs/2103.09329v1,"$K$-means clustering is one of the most widely-used partitioning algorithm incluster analysis due to its simplicity and computational efficiency. However,$K$-means does not provide an appropriate clustering result when applying todata with non-spherically shaped clusters. We propose a novel partitioningclustering algorithm based on expectiles. The cluster centers are defined asmultivariate expectiles and clusters are searched via a greedy algorithm byminimizing the within cluster '$\tau$ -variance'. We suggest two schemes: fixed$\tau$ clustering, and adaptive $\tau$ clustering. Validated by simulationresults, this method beats both $K$-means and spectral clustering on data withasymmetric shaped clusters, or clusters with a complicated structure, includingasymmetric normal, beta, skewed $t$ and $F$ distributed clusters. Applicationsof adaptive $\tau$ clustering on crypto-currency (CC) market data are provided.One finds that the expectiles clusters of CC markets show the phenomena of aninstitutional investors dominated market. The second application is on imagesegmentation. compared to other center based clustering methods, the adaptive$\tau$ cluster centers of pixel data can better capture and describe thefeatures of an image. The fixed $\tau$ clustering brings more flexibility onsegmentation with a decent accuracy.",ArXiv
Dynamic Grouping of Web Users Based on Their Web Access Patterns using  ART1 Neural Network Clustering Algorithm,"C. Ramya, G. Kavitha, K. S. Shreedhara",2012/05/09,http://arxiv.org/abs/1205.1938v1,"In this paper, we propose ART1 neural network clustering algorithm to groupusers according to their Web access patterns. We compare the quality ofclustering of our ART1 based clustering technique with that of the K-Means andSOM clustering algorithms in terms of inter-cluster and intra-clusterdistances. The results show the average inter-cluster distance of ART1 is highcompared to K-Means and SOM when there are fewer clusters. As the number ofclusters increases, average inter-cluster distance of ART1 is low compared toK-Means and SOM which indicates the high quality of clusters formed by ourapproach.",ArXiv
"To Cluster, or Not to Cluster: An Analysis of Clusterability Methods","A. Adolfsson, M. Ackerman, N. C. Brownstein",2018/08/24,http://arxiv.org/abs/1808.08317v1,"Clustering is an essential data mining tool that aims to discover inherentcluster structure in data. For most applications, applying clustering is onlyappropriate when cluster structure is present. As such, the study ofclusterability, which evaluates whether data possesses such structure, is anintegral part of cluster analysis. However, methods for evaluatingclusterability vary radically, making it challenging to select a suitablemeasure. In this paper, we perform an extensive comparison of measures ofclusterability and provide guidelines that clustering users can reference toselect suitable measures for their applications.",ArXiv
Observed Scaling Relations for Strong Lensing Clusters: Consequences for  Cosmology and Cluster Assembly,"Julia M. Comerford, Leonidas A. Moustakas, Priyamvada Natarajan",2010/04/05,http://arxiv.org/abs/1004.0694v1,"Scaling relations of observed galaxy cluster properties are useful tools forconstraining cosmological parameters as well as cluster formation histories.One of the key cosmological parameters, sigma8, is constrained using observedclusters of galaxies, although current estimates of sigma8 from the scalingrelations of dynamically relaxed galaxy clusters are limited by the largescatter in the observed cluster mass-temperature (M-T) relation. With a sampleof eight strong lensing clusters at 0.3 < z <0.8, we find that the observedcluster concentration-mass relation can be used to reduce the M-T scatter by afactor of 6. Typically only relaxed clusters are used to estimate sigma8, butcombining the cluster concentration-mass relation with the M-T relation enablesthe inclusion of unrelaxed clusters as well. Thus, the resultant gains in theaccuracy of sigma8 measurements from clusters are twofold: the errors on sigma8are reduced and the cluster sample size is increased. Therefore, the statisticson sigma8 determination from clusters are greatly improved by the inclusion ofunrelaxed clusters. Exploring cluster scaling relations further, we find thatthe correlation between brightest cluster galaxy (BCG) luminosity and clustermass offers insight into the assembly histories of clusters. We findpreliminary evidence for a steeper BCG luminosity - cluster mass relation forstrong lensing clusters than the general cluster population, hinting thatstrong lensing clusters may have had more active merging histories.",ArXiv
Tilting theory and cluster algebras,Idun Reiten,2010/12/29,http://arxiv.org/abs/1012.6014v1,"We give an introduction to the theory of cluster categories and clustertilted algebras. We include some background on the theory of cluster algebras,and discuss the interplay with cluster categories and cluster tilted algebras.",ArXiv
Deep Clustering With Consensus Representations,"Lukas Miklautz, Martin Teuffenbach, Pascal Weber, Rona Perjuci, Walid Durani, Christian Böhm, Claudia Plant",2022/10/13,http://arxiv.org/abs/2210.07063v1,"The field of deep clustering combines deep learning and clustering to learnrepresentations that improve both the learned representation and theperformance of the considered clustering method. Most existing deep clusteringmethods are designed for a single clustering method, e.g., k-means, spectralclustering, or Gaussian mixture models, but it is well known that no clusteringalgorithm works best in all circumstances. Consensus clustering tries toalleviate the individual weaknesses of clustering algorithms by building aconsensus between members of a clustering ensemble. Currently, there is no deepclustering method that can include multiple heterogeneous clustering algorithmsin an ensemble to update representations and clusterings together. To closethis gap, we introduce the idea of a consensus representation that maximizesthe agreement between ensemble members. Further, we propose DECCS (DeepEmbedded Clustering with Consensus representationS), a deep consensusclustering method that learns a consensus representation by enhancing theembedded space to such a degree that all ensemble members agree on a commonclustering result. Our contributions are the following: (1) We introduce theidea of learning consensus representations for heterogeneous clusterings, anovel notion to approach consensus clustering. (2) We propose DECCS, the firstdeep clustering method that jointly improves the representation and clusteringresults of multiple heterogeneous clustering algorithms. (3) We show inexperiments that learning a consensus representation with DECCS isoutperforming several relevant baselines from deep clustering and consensusclustering. Our code can be found at https://gitlab.cs.univie.ac.at/lukas/deccs",ArXiv
A score function for Bayesian cluster analysis,"John Noble, Łukasz Rajkowski",2019/05/24,http://arxiv.org/abs/1905.10209v1,We propose a score function for Bayesian clustering. The function isparameter free and captures the interplay between the within cluster varianceand the between cluster entropy of a clustering. It can be used to choose thenumber of clusters in well-established clustering methods such as hierarchicalclustering or $K$-means algorithm.,ArXiv
Review: Metaheuristic Search-Based Fuzzy Clustering Algorithms,"Waleed Alomoush, Ayat Alrosan",2018/01/21,http://arxiv.org/abs/1802.08729v1,"Fuzzy clustering is a famous unsupervised learning method used to collectingsimilar data elements within cluster according to some similarity measurement.But, clustering algorithms suffer from some drawbacks. Among the main weaknessincluding, selecting the initial cluster centres and the appropriate clustersnumber is normally unknown. These weaknesses are considered the mostchallenging tasks in clustering algorithms. This paper introduces acomprehensive review of metahueristic search to solve fuzzy clusteringalgorithms problems.",ArXiv
Some issues in robust clustering,Christian Hennig,2023/08/28,http://arxiv.org/abs/2308.14478v1,"Some key issues in robust clustering are discussed with focus on Gaussianmixture model based clustering, namely the formal definition of outliers,ambiguity between groups of outliers and clusters, the interaction betweenrobust clustering and the estimation of the number of clusters, the essentialdependence of (not only) robust clustering on tuning decisions, andshortcomings of existing measurements of cluster stability when it comes tooutliers.",ArXiv
The Cluster Substructure - Alignment Connection,Manolis Plionis,2001/05/30,http://arxiv.org/abs/astro-ph/0105522v1,"Using the APM cluster data we investigate whether the dynamical status ofclusters is related to the large-scale structure of the Universe. We find thatcluster substructure is strongly correlated with the tendency of clusters to bealigned with their nearest neighbour and in general with the nearby clustersthat belong to the same supercluster. Furthermore, dynamically young clustersare more clustered than the overall cluster population. These are strongindications that cluster develop in a hierarchical fashion by anisotropymerging along the large-scale filamentary superclusters within which they areembedded.",ArXiv
Experimental Estimation of Number of Clusters Based on Cluster Quality,"G. Hannah Grace, Kalyani Desikan",2015/03/10,http://arxiv.org/abs/1503.03168v1,"Text Clustering is a text mining technique which divides the given set oftext documents into significant clusters. It is used for organizing a hugenumber of text documents into a well-organized form. In the majority of theclustering algorithms, the number of clusters must be specified apriori, whichis a drawback of these algorithms. The aim of this paper is to showexperimentally how to determine the number of clusters based on clusterquality. Since partitional clustering algorithms are well-suited for clusteringlarge document datasets, we have confined our analysis to a partitionalclustering algorithm.",ArXiv
C-Planarity of Overlapping Clusterings Including Unions of Two  Partitions,"Jan Christoph Athenstädt, Sabine Cornelsen",2016/09/15,http://arxiv.org/abs/1609.04606v1,"We show that clustered planarity with overlapping clusters as introduced byDidimo et al. can be solved in polynomial time if each cluster induces aconnected subgraph. It can be solved in linear time if the set of clusters isthe union of two partitions of the vertex set such that, for each cluster, boththe cluster and its complement, induce connected subgraphs. Clustered planaritywith overlapping clusters is NP-complete, even if restricted to instances wherethe underlying graph is 2-connected, the set of clusters is the union of twopartitions and each cluster contains at most two connected components whiletheir complements contain at most three connected components.",ArXiv
Optical and X-ray clusters as tracers of the supercluster-void network.  I Superclusters of Abell and X-ray clusters,"M. Einasto, J. Einasto, E. Tago, V. M""uller, H. Andernach",2000/12/29,http://arxiv.org/abs/astro-ph/0012536v1,"We study the distribution of X-ray selected clusters of galaxies with respectto superclusters determined by Abell clusters of galaxies and show that thedistribution of X-ray clusters follows the supercluster-void network determinedby Abell clusters. We find that in this network X-ray clusters are morestrongly clustered than other clusters. Poor, non-Abell X-ray clusters followthe supercluster-void network as well: these clusters are embedded insuperclusters determined by rich clusters and populate filaments between them.We present a new catalog of superclusters of Abell clusters out to a redshiftof z_{lim}=0.13, a catalog of X-ray clusters located in superclustersdetermined by Abell clusters, and a list of additional superclusters of X-rayclusters.",ArXiv
Generalization of Clustering Agreements and Distances for Overlapping  Clusters and Network Communities,"Reihaneh Rabbany, Osmar R. Zaïane",2014/12/08,http://arxiv.org/abs/1412.2601v2,"A measure of distance between two clusterings has important applications,including clustering validation and ensemble clustering. Generally, suchdistance measure provides navigation through the space of possible clusterings.Mostly used in cluster validation, a normalized clustering distance, a.k.a.agreement measure, compares a given clustering result against the ground-truthclustering. Clustering agreement measures are often classified into twofamilies of pair-counting and information theoretic measures, with thewidely-used representatives of Adjusted Rand Index (ARI) and Normalized MutualInformation (NMI), respectively. This paper sheds light on the relation betweenthese two families through a generalization. It further presents an alternativealgebraic formulation for these agreement measures which incorporates anintuitive clustering distance, which is defined based on the analogous betweencluster overlaps and co-memberships of nodes in clusters. Unlike the originalmeasures, it is easily extendable for different cases, including overlappingclusters and clusters of inter-related data for complex networks. These twoextensions are, in particular, important in the context of finding clusters insocial and information networks, a.k.a communities.",ArXiv
Clustering strategy and method selection,Christian Hennig,2015/03/06,http://arxiv.org/abs/1503.02059v1,"This paper is a chapter in the forthcoming Handbook of Cluster Analysis,Hennig et al. (2015). For definitions of basic clustering methods and somefurther methodology, other chapters of the Handbook are referred to. To readthis version of the paper without the Handbook, some knowledge of clusteranalysis methodology is required.  The aim of this chapter is to provide a framework for all the decisions thatare required when carrying out a cluster analysis in practice. A generalattitude to clustering is outlined, which connects these decisions closely tothe clustering aims in a given application. From this point of view, thechapter then discusses aspects of data processing such as the choice of therepresentation of the objects to be clustered, dissimilarity design,transformation and standardization of variables. Regarding the choice of theclustering method, it is explored how different methods correspond to differentclustering aims. Then an overview of benchmarking studies comparing differentclustering methods is given, as well as an out- line of theoretical approachesto characterize desiderata for clustering by axioms. Finally, aspects ofcluster validation, i.e., the assessment of the quality of a clustering in agiven dataset, are discussed, including finding an appropriate number ofclusters, testing homogeneity, internal and external cluster validation,assessing clustering stability and data visualization.",ArXiv
Communication-Efficient and Exact Clustering Distributed Streaming Data,Dang-Hoan Tran,2012/09/19,http://arxiv.org/abs/1209.4257v1,"A widely used approach to clustering a single data stream is the two-phasedapproach in which the online phase creates and maintains micro-clusters whilethe off-line phase generates the macro-clustering from the micro-clusters. Weuse this approach to propose a distributed framework for clustering streamingdata. Our proposed framework consists of fundamen- tal processes: onecoordinator-site process and many remote-site processes. Remote-site processescan directly communicate with the coordinator-process but cannot communicatethe other remote site processes. Every remote-site process generates andmaintains micro- clusters that represent cluster information summary, from itslocal data stream. Remote sites send the local micro-clusterings to thecoordinator by the serialization technique, or the coordinator invokes theremote methods in order to get the local micro-clusterings from the remotesites. After the coordinator receives all the local micro-clusterings from theremote sites, it generates the global clustering by the macro-clusteringmethod. Our theoretical and empirical results show that, the global clusteringgenerated by our distributed framework is similar to the clustering generatedby the underlying centralized algorithm on the same data set. By using thelocal micro-clustering approach, our framework achieves high scalability, andcommunication-efficiency.",ArXiv
Comparing clusterings and numbers of clusters by aggregation of  calibrated clustering validity indexes,"Serhat Emre Akhanli, Christian Hennig",2020/02/05,http://arxiv.org/abs/2002.01822v4,"A key issue in cluster analysis is the choice of an appropriate clusteringmethod and the determination of the best number of clusters. Differentclusterings are optimal on the same data set according to different criteria,and the choice of such criteria depends on the context and aim of clustering.Therefore, researchers need to consider what data analytic characteristics theclusters they are aiming at are supposed to have, among others within-clusterhomogeneity, between-clusters separation, and stability. Here, a set ofinternal clustering validity indexes measuring different aspects of clusteringquality is proposed, including some indexes from the literature. Users canchoose the indexes that are relevant in the application at hand. In order tomeasure the overall quality of a clustering (for comparing clusterings fromdifferent methods and/or different numbers of clusters), the index values arecalibrated for aggregation. Calibration is relative to a set of randomclusterings on the same data. Two specific aggregated indexes are proposed andcompared with existing indexes on simulated and real data.",ArXiv
Convex Clustering through MM: An Efficient Algorithm to Perform  Hierarchical Clustering,"Daniel J. W. Touw, Patrick J. F. Groenen, Yoshikazu Terada",2022/11/03,http://arxiv.org/abs/2211.01877v2,"Convex clustering is a modern method with both hierarchical and $k$-meansclustering characteristics. Although convex clustering can capture complexclustering structures hidden in data, the existing convex clustering algorithmsare not scalable to large data sets with sample sizes greater than severalthousands. Moreover, it is known that convex clustering sometimes fails toproduce a complete hierarchical clustering structure. This issue arises ifclusters split up or the minimum number of possible clusters is larger than thedesired number of clusters. In this paper, we propose convex clustering throughmajorization-minimization (CCMM) -- an iterative algorithm that uses clusterfusions and a highly efficient updating scheme derived using diagonalmajorization. Additionally, we explore different strategies to ensure that thehierarchical clustering structure terminates in a single cluster. With acurrent desktop computer, CCMM efficiently solves convex clustering problemsfeaturing over one million objects in seven-dimensional space, achieving asolution time of 51 seconds on average.",ArXiv
On the Formation of cD Galaxies and their Parent Clusters,"H. M. Tovmassian, H. Andernach",2012/12/02,http://arxiv.org/abs/1212.0238v1,"In order to study the mechanism of formation of cD galaxies we search forpossible dependencies between the K-band luminosity of cDs and the parametersof their host clusters which we select to have a dominant cD galaxy,corresponding to a cluster morphology of Bautz-Morgan (BM) type I. As acomparison sample we use cD galaxies in clusters where they are not dominant,which we define here as non-BMI (NBMI) type clusters. We find that for 71 BMIclusters the absolute K-band luminosity of cDs depends on the cluster richness,but less strongly on the cluster velocity dispersion. Meanwhile, for 35 NBMIclusters the correlation between cD luminosity and cluster richness is weaker,and is absent between cD luminosity and velocity dispersion. In addition, wefind that the luminosity of the cD galaxy hosted in BMI clusters tends toincrease with the cD's peculiar velocity with respect to the cluster meanvelocity. In contrast, for NBMI clusters the cD luminosity decreases withincreasing peculiar velocity. Also, the X-ray luminosity of BMI clustersdepends on the cluster velocity dispersion, while in NBMI clusters such acorrelation is absent. These findings favour the cannibalism scenario for theformation of cD galaxies. We suggest that cDs in clusters of BMI type wereformed and evolved preferentially in one and the same cluster. In contrast, cDsin NBMI type clusters were either originally formed in clusters that latermerged with groups or clusters to form the current cluster, or are now in theprocess of merging.",ArXiv
A catalog of newly identified star clusters in GAIA DR2,"Lei Liu, Xiaoying Pang",2019/10/28,http://arxiv.org/abs/1910.12600v1,"We present the Star cluster Hunting Pipeline (SHiP) which can identify starclusters in GAIA DR2 data, and establish a star cluster catalog for theGalactic disk. A Friend of Friend based cluster finder method is used toidentify star clusters using 5-dimensional stellar parameters, $l, b, \varpi,\mu_\alpha\cos\delta$, and $\mu_\delta$. Our new catalog contains 2443 starcluster candidates identified from disk stars located within $|b|=25^\circ$ andwith $G<18$ mag. An automatic isochrone fitting scheme is applied to allcluster candidates. With a combination of parameters obtained from isochronefitting, we classify cluster candidates into three classes (Class 1, 2 and 3).Class 1 clusters are the most probable star cluster candidates with the moststringent criteria. Most of these clusters are nearby (within 4 kpc). Ourcatalog is cross-matched with three Galactic star cluster catalogs, Kharchenkoet al. (2013), Cantat-Gaudin et al. (2018, 2019), and Bica et al. (2019). Theproper motion and parallax of matched star clusters are in good agreement withthese earlier catalogs. We discover 76 new star cluster candidates that are notlisted in these 3 catalogs. The majority of these are clusters older thanlog(age/yr)=8.0, and are located in the inner disk with $|b|<5^\circ$. Therecent discovery of new star clusters suggests that current Galactic starcluster catalogs are still incomplete. Among the Class 1 cluster candidates, wefind 56 candidates for star cluster groups. The pipeline, the catalog and themember list containing all candidates star clusters and star cluster groupshave been made publicly available.",ArXiv
Reanalysis of nearby open clusters using Gaia DR1/TGAS and HSOY,"Steffi X. Yen, Sabine Reffert, Elena Schilbach, Siegfried Röser, Nina V. Kharchenko, Anatoly E. Piskunov",2018/02/12,http://arxiv.org/abs/1802.04234v1,"Open clusters have long been used to gain insights into the structure,composition, and evolution of the Galaxy. With the large amount of stellar dataavailable for many clusters in the Gaia era, new techniques must be developedfor analyzing open clusters, as visual inspection of cluster color-magnitudediagrams is no longer feasible. An automatic tool will be required to analyzelarge samples of open clusters. We seek to develop an automaticisochrone-fitting procedure to consistently determine cluster membership andthe fundamental cluster parameters. Our cluster characterization pipeline firstdetermined cluster membership with precise astrometry, primarily from TGAS andHSOY. With initial cluster members established, isochrones were fitted, using achi-squared minimization, to the cluster photometry in order to determinecluster mean distances, ages, and reddening. Cluster membership was alsorefined based on the stellar photometry. We used multiband photometry, whichincludes ASCC-2.5 BV, 2MASS JHK_s, Gaia G band. We present parameter estimatesfor all 24 clusters closer than 333 pc as determined by the Catalogue of OpenCluster Data and the Milky Way Star Clusters catalog. We find that ourparameters are consistent to those in the Milky Way Star Clusters catalog. Wedemonstrate that it is feasible to develop an automated pipeline thatdetermines cluster parameters and membership reliably. After additionalmodifications, our pipeline will be able to use Gaia DR2 as input, leading tobetter cluster memberships and more accurate cluster parameters for a muchlarger number of clusters.",ArXiv
The Impact of Random Models on Clustering Similarity,"Alexander J Gates, Yong-Yeol Ahn",2017/01/23,http://arxiv.org/abs/1701.06508v2,"Clustering is a central approach for unsupervised learning. After clusteringis applied, the most fundamental analysis is to quantitatively compareclusterings. Such comparisons are crucial for the evaluation of clusteringmethods as well as other tasks such as consensus clustering. It is often arguedthat, in order to establish a baseline, clustering similarity should beassessed in the context of a random ensemble of clusterings. The prevailingassumption for the random clustering ensemble is the permutation model in whichthe number and sizes of clusters are fixed. However, this assumption does notnecessarily hold in practice; for example, multiple runs of K-means clusteringreturns clusterings with a fixed number of clusters, while the cluster sizedistribution varies greatly. Here, we derive corrected variants of twoclustering similarity measures (the Rand index and Mutual Information) in thecontext of two random clustering ensembles in which the number and sizes ofclusters vary. In addition, we study the impact of one-sided comparisons in thescenario with a reference clustering. The consequences of different randommodels are illustrated using synthetic examples, handwriting recognition, andgene expression data. We demonstrate that the choice of random model can have adrastic impact on the ranking of similar clustering pairs, and the evaluationof a clustering method with respect to a random baseline; thus, the choice ofrandom clustering model should be carefully justified.",ArXiv
On the Persistence of Clustering Solutions and True Number of Clusters  in a Dataset,"Amber Srivastava, Mayank Baranwal, Srinivasa Salapaka",2018/10/31,http://arxiv.org/abs/1811.00102v2,"Typically clustering algorithms provide clustering solutions withprespecified number of clusters. The lack of a priori knowledge on the truenumber of underlying clusters in the dataset makes it important to have ametric to compare the clustering solutions with different number of clusters.This article quantifies a notion of persistence of clustering solutions thatenables comparing solutions with different number of clusters. The persistencerelates to the range of data-resolution scales over which a clustering solutionpersists; it is quantified in terms of the maximum over two-norms of all theassociated cluster-covariance matrices. Thus we associate a persistence valuefor each element in a set of clustering solutions with different number ofclusters. We show that the datasets where natural clusters are a priori known,the clustering solutions that identify the natural clusters are most persistent- in this way, this notion can be used to identify solutions with true numberof clusters. Detailed experiments on a variety of standard and syntheticdatasets demonstrate that the proposed persistence-based indicator outperformsthe existing approaches, such as, gap-statistic method, $X$-means, $G$-means,$PG$-means, dip-means algorithms and information-theoretic method, inaccurately identifying the clustering solutions with true number of clusters.Interestingly, our method can be explained in terms of the phase-transitionphenomenon in the deterministic annealing algorithm, where the number ofdistinct cluster centers changes (bifurcates) with respect to an annealingparameter.",ArXiv
Oracle-guided Contrastive Clustering,"Mengdie Wang, Liyuan Shang, Suyun Zhao, Yiming Wang, Hong Chen, Cuiping Li, Xizhao Wang",2022/11/01,http://arxiv.org/abs/2211.00409v1,"Deep clustering aims to learn a clustering representation through deeparchitectures. Most of the existing methods usually conduct clustering with theunique goal of maximizing clustering performance, that ignores the personalizeddemand of clustering tasks.% and results in unguided clustering solutions.However, in real scenarios, oracles may tend to cluster unlabeled data byexploiting distinct criteria, such as distinct semantics (background, color,object, etc.), and then put forward personalized clustering tasks. To achievetask-aware clustering results, in this study, Oracle-guided ContrastiveClustering(OCC) is then proposed to cluster by interactively making pairwise``same-cluster"" queries to oracles with distinctive demands. Specifically,inspired by active learning, some informative instance pairs are queried, andevaluated by oracles whether the pairs are in the same cluster according totheir desired orientation. And then these queried same-cluster pairs extend theset of positive instance pairs for contrastive learning, guiding OCC to extractorientation-aware feature representation. Accordingly, the query results,guided by oracles with distinctive demands, may drive the OCC's clusteringresults in a desired orientation. Theoretically, the clustering risk in anactive learning manner is given with a tighter upper bound, that guaranteesactive queries to oracles do mitigate the clustering risk. Experimentally,extensive results verify that OCC can cluster accurately along the specificorientation and it substantially outperforms the SOTA clustering methods aswell. To the best of our knowledge, it is the first deep framework to performpersonalized clustering.",ArXiv
Learning Uniform Clusters on Hypersphere for Deep Graph-level Clustering,"Mengling Hu, Chaochao Chen, Weiming Liu, Xinyi Zhang, Xinting Liao, Xiaolin Zheng",2023/11/23,http://arxiv.org/abs/2311.13953v1,"Graph clustering has been popularly studied in recent years. However, mostexisting graph clustering methods focus on node-level clustering, i.e.,grouping nodes in a single graph into clusters. In contrast, graph-levelclustering, i.e., grouping multiple graphs into clusters, remains largelyunexplored. Graph-level clustering is critical in a variety of real-worldapplications, such as, properties prediction of molecules and communityanalysis in social networks. However, graph-level clustering is challenging dueto the insufficient discriminability of graph-level representations, and theinsufficient discriminability makes deep clustering be more likely to obtaindegenerate solutions (cluster collapse). To address the issue, we propose anovel deep graph-level clustering method called Uniform Deep Graph Clustering(UDGC). UDGC assigns instances evenly to different clusters and then scattersthose clusters on unit hypersphere, leading to a more uniform cluster-leveldistribution and a slighter cluster collapse. Specifically, we first proposeAugmentation-Consensus Optimal Transport (ACOT) for generating uniformlydistributed and reliable pseudo labels for partitioning clusters. Then we adoptcontrastive learning to scatter those clusters. Besides, we propose CenterAlignment Optimal Transport (CAOT) for guiding the model to learn betterparameters, which further promotes the cluster performance. Our empirical studyon eight well-known datasets demonstrates that UDGC significantly outperformsthe state-of-the-art models.",ArXiv
Polar Chemoreceptor Clustering by Coupled Trimers of Dimers,Robert G. Endres,2009/06/11,http://arxiv.org/abs/0906.2145v1,"Receptors of bacterial chemotaxis form clusters at the cell poles, whereclusters act as ""antennas"" to amplify small changes in ligand concentration.Interestingly, chemoreceptors cluster at multiple length scales. At thesmallest scale, receptors form dimers, which assemble into stable timers ofdimers. At a large scale, trimers form large polar clusters composed ofthousands of receptors. Although much is known about the signaling propertiesemerging from receptor clusters, it is unknown how receptors localize at thecell poles and what the cluster-size determining factors are. Here, we presenta model of polar receptor clustering based on coupled trimers of dimers, wherecluster size is determined as a minimum of the cluster-membrane free energy.This energy has contributions from the cluster-membrane elastic energy,penalizing large clusters due to their high intrinsic curvature, andreceptor-receptor coupling favoring large clusters. We find that the reducedcluster-membrane curvature mismatch at the curved cell poles leads to large androbust polar clusters in line with experimental observation, while lateralclusters are efficiently suppressed.",ArXiv
Performance Comparison of Two Streaming Data Clustering Algorithms,"Chandrakant Mahobiya, M. Kumar",2014/06/26,http://arxiv.org/abs/1406.6778v1,The weighted fuzzy c-mean clustering algorithm and weighted fuzzyc-mean-adaptive cluster number are extension of traditional fuzzy c-meanAlgorithm to stream data clustering algorithm.,ArXiv
Cluster automorphism groups of cluster algebras of finite type,"Wen Chang, Bin Zhu",2015/06/05,http://arxiv.org/abs/1506.01950v2,"We study the cluster automorphism group $Aut(\mathcal{A})$ of a coefficientfree cluster algebra $\mathcal{A}$ of finite type. A cluster automorphism of$\mathcal{A}$ is a permutation of the cluster variable set $\mathscr{X}$ thatis compatible with cluster mutations. We show that, on the one hand, by thewell-known correspondence between $\mathscr{X}$ and the almost positive rootsystem $\Phi_{\geq -1}$ of the corresponding Dynkin type, the piecewise-lineartransformations $\tau_+$ and $\tau_-$ on $\Phi_{\geq -1}$ induce clusterautomorphisms $f_+$ and $f_-$ of $\mathcal{A}$ respectively; on the other hand,excepting type $D_{2n},(n\geqslant 2)$, all the cluster automorphisms of$\mathcal{A}$ are compositions of $f_+$ and $f_-$. For a cluster algebra oftype $D_{2n},(n\geqslant 2)$, there exists exceptional cluster automorphisminduced by a permutation of negative simple roots in $\Phi_{\geq -1}$, which isnot a composition of $\tau_+$ and $\tau_-$. By using these results and foldinga simply laced cluster algebra, we compute the cluster automorphism group for anon-simply laced finite type cluster algebra. As an application, we show that$Aut(\mathcal{A})$ is isomorphic to the cluster automorphism group of the$FZ$-universal cluster algebra of $\mathcal{A}$.",ArXiv
Self-supervised Contrastive Attributed Graph Clustering,"Wei Xia, Quanxue Gao, Ming Yang, Xinbo Gao",2021/10/15,http://arxiv.org/abs/2110.08264v1,"Attributed graph clustering, which learns node representation from nodeattribute and topological graph for clustering, is a fundamental butchallenging task for graph analysis. Recently, methods based on graphcontrastive learning (GCL) have obtained impressive clustering performance onthis task. Yet, we observe that existing GCL-based methods 1) fail to benefitfrom imprecise clustering labels; 2) require a post-processing operation to getclustering labels; 3) cannot solve out-of-sample (OOS) problem. To addressthese issues, we propose a novel attributed graph clustering network, namelySelf-supervised Contrastive Attributed Graph Clustering (SCAGC). In SCAGC, byleveraging inaccurate clustering labels, a self-supervised contrastive loss,which aims to maximize the similarities of intra-cluster nodes while minimizingthe similarities of inter-cluster nodes, are designed for node representationlearning. Meanwhile, a clustering module is built to directly output clusteringlabels by contrasting the representation of different clusters. Thus, for theOOS nodes, SCAGC can directly calculate their clustering labels. Extensiveexperimental results on four benchmark datasets have shown that SCAGCconsistently outperforms 11 competitive clustering methods.",ArXiv
CNAK : Cluster Number Assisted K-means,"Jayasree Saha, Jayanta Mukherjee",2019/11/20,http://arxiv.org/abs/1911.08871v1,"Determining the number of clusters present in a dataset is an importantproblem in cluster analysis. Conventional clustering techniques generallyassume this parameter to be provided up front. %user supplied. %Recently,robustness of any given clustering algorithm is analyzed to measure clusterstability/instability which in turn determines the cluster number. In thispaper, we propose a method which analyzes cluster stability for predicting thecluster number. Under the same computational framework, the technique alsofinds representatives of the clusters. The method is apt for handling big data,as we design the algorithm using \emph{Monte-Carlo} simulation. Also, weexplore a few pertinent issues found to be of also clustering. Experimentsreveal that the proposed method is capable of identifying a single cluster. Itis robust in handling high dimensional dataset and performs reasonably wellover datasets having cluster imbalance. Moreover, it can indicate clusterhierarchy, if present. Overall we have observed significant improvement inspeed and quality for predicting cluster numbers as well as the composition ofclusters in a large dataset.",ArXiv
Deep Clustering: A Comprehensive Survey,"Yazhou Ren, Jingyu Pu, Zhimeng Yang, Jie Xu, Guofeng Li, Xiaorong Pu, Philip S. Yu, Lifang He",2022/10/09,http://arxiv.org/abs/2210.04142v1,"Cluster analysis plays an indispensable role in machine learning and datamining. Learning a good data representation is crucial for clusteringalgorithms. Recently, deep clustering, which can learn clustering-friendlyrepresentations using deep neural networks, has been broadly applied in a widerange of clustering tasks. Existing surveys for deep clustering mainly focus onthe single-view fields and the network architectures, ignoring the complexapplication scenarios of clustering. To address this issue, in this paper weprovide a comprehensive survey for deep clustering in views of data sources.With different data sources and initial conditions, we systematicallydistinguish the clustering methods in terms of methodology, prior knowledge,and architecture. Concretely, deep clustering methods are introduced accordingto four categories, i.e., traditional single-view deep clustering,semi-supervised deep clustering, deep multi-view clustering, and deep transferclustering. Finally, we discuss the open challenges and potential futureopportunities in different fields of deep clustering.",ArXiv
DivClust: Controlling Diversity in Deep Clustering,"Ioannis Maniadis Metaxas, Georgios Tzimiropoulos, Ioannis Patras",2023/04/03,http://arxiv.org/abs/2304.01042v1,"Clustering has been a major research topic in the field of machine learning,one to which Deep Learning has recently been applied with significant success.However, an aspect of clustering that is not addressed by existing deepclustering methods, is that of efficiently producing multiple, diversepartitionings for a given dataset. This is particularly important, as a diverseset of base clusterings are necessary for consensus clustering, which has beenfound to produce better and more robust results than relying on a singleclustering. To address this gap, we propose DivClust, a diversity controllingloss that can be incorporated into existing deep clustering frameworks toproduce multiple clusterings with the desired degree of diversity. We conductexperiments with multiple datasets and deep clustering frameworks and showthat: a) our method effectively controls diversity across frameworks anddatasets with very small additional computational cost, b) the sets ofclusterings learned by DivClust include solutions that significantly outperformsingle-clustering baselines, and c) using an off-the-shelf consensus clusteringalgorithm, DivClust produces consensus clustering solutions that consistentlyoutperform single-clustering baselines, effectively improving the performanceof the base deep clustering framework.",ArXiv
Detecting outliers by clustering algorithms,"Qi Li, Shuliang Wang",2024/12/07,http://arxiv.org/abs/2412.05669v1,"Clustering and outlier detection are two important tasks in data mining.Outliers frequently interfere with clustering algorithms to determine thesimilarity between objects, resulting in unreliable clustering results.Currently, only a few clustering algorithms (e.g., DBSCAN) have the ability todetect outliers to eliminate interference. For other clustering algorithms, itis tedious to introduce another outlier detection task to eliminate outliersbefore each clustering process. Obviously, how to equip more clusteringalgorithms with outlier detection ability is very meaningful. Although a commonstrategy allows clustering algorithms to detect outliers based on the distancebetween objects and clusters, it is contradictory to improving the performanceof clustering algorithms on the datasets with outliers. In this paper, wepropose a novel outlier detection approach, called ODAR, for clustering. ODARmaps outliers and normal objects into two separated clusters by featuretransformation. As a result, any clustering algorithm can detect outliers byidentifying clusters. Experiments show that ODAR is robust to diverse datasets.Compared with baseline methods, the clustering algorithms achieve the best on 7out of 10 datasets with the help of ODAR, with at least 5% improvement inaccuracy.",ArXiv
Extension of fixed point clustering: A cluster criterion,"A. Hutt, F. Kruggel",2001/02/24,http://arxiv.org/abs/nlin/0102032v1,"The present report extends the method of fixed point clustering (Phys.Rev. E61,5, R4691-4693, 2000) by introducing an indirect criterion for the number ofclusters. The derived probability function allows an objective distinction ofclustered data and data between clusters. Applications on simulated dataillustrate the clustering method and the probability function.",ArXiv
Building dwarf galaxies out of merged young star clusters,M. Fellhauer,2001/06/22,http://arxiv.org/abs/astro-ph/0106416v1,"Young star clusters in interacting galaxies are often found in groups orclusters of star clusters containing up to 100 single clusters. In our projectwe study the future fate of these clusters of star clusters. We find that thestar clusters merge on time scales of a few dynamical crossing times of thesuper-cluster. The resulting merger object has similarities with observed dwarfellipticals (dE). Furthermore, if destructive processes like tidal heating,dynamical friction or interaction with disc or bulge of the parent galaxy aretaken into account our merger objects may evolve into objects resembling dwarfspheroidal galaxies (dSph), without the need of a high dark matter content.",ArXiv
The Large-Scale Environment of Dynamical Young Clusters of Galaxies,"M. Plionis, S. Basilakos",2001/10/02,http://arxiv.org/abs/astro-ph/0110063v1,"We investigate whether the dynamical status of clusters is related to thelarge-scale structure of the Universe. We find that cluster substructure isstrongly correlated with the tendency of clusters to be aligned with theirnearest neighbour and in general with the nearby clusters that belong to thesame supercluster. Furthermore, dynamically young clusters are more clusteredthan the overall cluster population. These are strong indications that clustersdevelop in a hierarchical fashion by merging along the large-scale filamentarystructures within which they are embedded.",ArXiv
Large-Scale Environmental Effects of the Cluster Distribution,Manolis Plionis,2001/10/29,http://arxiv.org/abs/astro-ph/0110616v1,"Using the APM cluster distribution we find interesting alignment effects: (1)Cluster substructure is strongly correlated with the tendency of clusters to bealigned with their nearest neighbour and in general with the nearby clustersthat belong to the same supercluster, (2) Clusters belonging in superclustersshow a statistical significant tendency to be aligned with the major axisorientation of their parent supercluster. Furthermore we find that dynamicallyyoung clusters are more clustered than the overall cluster population. Theseare strong indications that cluster develop in a hierarchical fashion bymerging along the large-scale filamentary superclusters within which they areembedded.",ArXiv
Macrostate Data Clustering,"Daniel Korenblum, David Shalloway",2003/06/19,http://arxiv.org/abs/physics/0306145v1,"We develop an effective nonhierarchical data clustering method using ananalogy to the dynamic coarse graining of a stochastic system. Analyzing theeigensystem of an interitem transition matrix identifies fuzzy clusterscorresponding to the metastable macroscopic states (macrostates) of a diffusivesystem. A ""minimum uncertainty criterion"" determines the linear transformationfrom eigenvectors to cluster-defining window functions. Eigenspectrum gap andcluster certainty conditions identify the proper number of clusters. Thephysically motivated fuzzy representation and associated uncertainty analysisdistinguishes macrostate clustering from spectral partitioning methods.Macrostate data clustering solves a variety of test cases that challenge othermethods.",ArXiv
On cluster algebras arising from unpunctured surfaces,"Ralf Schiffler, Hugh Thomas",2007/12/26,http://arxiv.org/abs/0712.4131v2,"We study cluster algebras that are associated to unpunctured surfaces withcoefficients arising from boundary arcs. We give a direct formula for theLaurent polynomial expansion of cluster variables in these cluster algebras interms of certain paths on a triangulation of the surface. As an immediateconsequence, we prove the positivity conjecture of Fomin and Zelevinsky forthese cluster algebras. In the special case where the cluster algebra isacyclic, we also give a formula for the expansion of cluster variables as apolynomial whose indeterminates are the cluster variables contained in theunion of an arbitrary acyclic cluster and all its neighbouring clusters in themutation graph.",ArXiv
F-polynomials in Quantum Cluster Algebras,Thao Tran,2009/04/21,http://arxiv.org/abs/0904.3291v1,"F-polynomials and g-vectors were defined by Fomin and Zelevinsky to give aformula which expresses cluster variables in a cluster algebra in terms of theinitial cluster data. A quantum cluster algebra is a certain noncommutativedeformation of a cluster algebra. In this paper, we define and prove theexistence of analogous quantum F-polynomials for quantum cluster algebras. Weprove some properties of quantum F-polynomials. In particular, we give arecurrence relation which can be used to compute them. Finally, we computequantum F-polynomials and g-vectors for a certain class of cluster variables,which includes all cluster variables in type A quantum cluster algebras.",ArXiv
Simulating Galaxy Clusters,Michael L. Norman,2010/05/06,http://arxiv.org/abs/1005.1100v1,"This pedagogical review of galaxy cluster simulations is based on threelectures given at the 2008 Enrico Fermi Summer School entitled ""Astrophysics ofGalaxy Clusters"". It covers the standard cosmological framework, growth ofperturbations in the linear regime, analytic models for nonlinear perturbationgrowth, statistics of galaxy cluster populations, virial scaling relations,overview of numerical methods, simulating gas in galaxy clusters, basic resultson adiabatic clusters (Santa Barbara cluster comparison project), effect ofadditional physics, recent progress in galaxy clustering modeling (Galcons,turbulence, AGN jets, cluster-wide B-fields), simulating statistical samplesand lightcones, and simulated SZE surveys.",ArXiv
Cluster automorphisms and compatibility of cluster variables,"Ibrahim Assem, Ralf Schiffler, Vasilisa Shramchenko",2013/07/18,http://arxiv.org/abs/1307.4838v1,"In this paper, we introduce a notion of unistructural cluster algebras, forwhich the set of cluster variables uniquely determines the clusters. We provethat cluster algebras of Dynkin type and cluster algebras of rank 2 areunistructural, then prove that if $\mathcal{A}$ is unistructural or ofEuclidean type, then $f: \mathcal{A}\to \mathcal{A}$ is a cluster automorphismif and only if $f$ is an automorphism of the ambient field which restricts to apermutation of the cluster variables. In order to prove this result, we alsoinvestigate the Fomin-Zelevinsky conjecture that two cluster variables arecompatible if and only if one does not appear in the denominator of the Laurentexpansions of the other.",ArXiv
Dynamic Cluster Head Selection Using Fuzzy Logic on Cloud in Wireless  Sensor Networks,"Payal Pahwa, Deepali Virmani, Akshay Kumar, Sahil, Vikas Rathi, Sunil Swami",2015/11/03,http://arxiv.org/abs/1601.03810v1,"One of the most vital activities to reduce energy consumption in wirelesssensor networks is clustering. In clustering, one node from a group of nodes isselected to be a cluster head, which handles majority of the computation andprocessing for the nodes in the cluster. This paper proposes an algorithm forfuzzy based dynamic cluster head selection on cloud in wireless sensornetworks. The proposed algorithm calculates a Potential value for each node andselects cluster heads with high potential. The proposed algorithm minimizescluster overlapping by spatial distribution of cluster heads and discardsmalicious nodes i.e. never allows malicious nodes to be cluster heads.",ArXiv
Component cluster for acyclic quiver,Sarah Scherotzke,2015/06/21,http://arxiv.org/abs/1506.06327v1,"The theory of Caldero-Chapoton algebras of Cerulli-Irelli, Labardini-Fragosoand Schroer leads to a refinement of the notions of cluster variables andclusters, via so called component clusters. In this paper we compare componentclusters to classical clusters for the cluster algebra of an acyclic quiver. Wepropose a definition of mutation between component clusters and determine themutation relations of component clusters for affine quivers. In the case of awild quiver, we provide bounds for the size of component clusters.",ArXiv
Skeleton Clustering: Dimension-Free Density-based Clustering,"Zeyu Wei, Yen-Chi Chen",2021/04/21,http://arxiv.org/abs/2104.10770v2,"We introduce a density-based clustering method called skeleton clusteringthat can detect clusters in multivariate and even high-dimensional data withirregular shapes. To bypass the curse of dimensionality, we propose surrogatedensity measures that are less dependent on the dimension but have intuitivegeometric interpretations. The clustering framework constructs a conciserepresentation of the given data as an intermediate step and can be thought ofas a combination of prototype methods, density-based clustering, andhierarchical clustering. We show by theoretical analysis and empirical studiesthat the skeleton clustering leads to reliable clusters in multivariate andhigh-dimensional scenarios.",ArXiv
Cluster Theories and Cluster Structures of Type A,Job Daisie Rock,2021/12/29,http://arxiv.org/abs/2112.14795v1,"In the present paper we examine the relationship between several type $A$cluster theories and structures. We define a 2D geometric model of a clustertheory, which generalizes cluster algebras from surfaces, and encode severalexisting type $A$ cluster theories into a 2D geometric model. We review twoother cluster theories of type $A$. Then we introduce an abstraction of clusterstructures. We prove two results: the first relates several existing type $A$cluster theories and the second relates some of these cluster structures usingthe new abstraction.",ArXiv
